{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbc1728",
   "metadata": {},
   "source": [
    "# Weak Supervision with Snorkel AI\n",
    "\n",
    "Create labels for unlabeled data based on some heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec884138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\22156\\Anaconda3\\envs\\aml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# Snorkel AI weak supervision\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel\n",
    "# classification model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "# SHAP explainability\n",
    "import shap\n",
    "# save model\n",
    "import pickle\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# directories\n",
    "RAW_DIR = 'data/raw'\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "CHECKPOINT_DIR = 'checkpoint'\n",
    "\n",
    "# weak label constants\n",
    "ABSTAIN = -1  # Uncertain\n",
    "NEGATIVE = 0  # Low risk\n",
    "POSITIVE = 1  # High risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd582a8c",
   "metadata": {},
   "source": [
    "## Part 1: Define Snorkel Labeling Functions (LFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd61387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def high_txn_volume_vs_income(x):\n",
    "    \"\"\"High transaction volume relative to income (suspicious if transaction volume is much higher than income)\"\"\"\n",
    "    condition = (x.txn_volume_vs_income > 2477.8) & ((x.age <= 25) | (x.income <= 40000))\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_txn_volume_vs_occupation_median(x):\n",
    "    \"\"\"Transaction volume vs occupation median (if transaction volume is high compared to typical for the occupation)\"\"\"\n",
    "    condition = (x.txn_volume_vs_occupation_median > 1562.0) & ((x.age <= 25) | (x.income <= 40000))\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_wire(x):\n",
    "    \"\"\"High wire transfer amounts if income or age is low (could be mule)\"\"\"\n",
    "    condition = (x.median_amt_wire > 6817.2) & ((x.age <= 25) | (x.income <= 40000))\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def frequent_wire(x):\n",
    "    \"\"\"Frequent wire transfer amounts if income or age is low\"\"\"\n",
    "    condition = (x.wire_ratio > 0.47) & ((x.age <= 25) | (x.income <= 40000))\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def structuring(x):\n",
    "    \"\"\"Structuring (frequent transactions just below the reporting threshold)\"\"\"\n",
    "    condition = (x.count_txn_below_threshold_frequency > 0.009) & (x.n_txn_total > 5)\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def short_hold_time(x):\n",
    "    \"\"\"Rapid movement of funds (short hold time)\"\"\"\n",
    "    condition = (x.n_txn_total > 50) & (x.median_hold_time_funds <= 0.11)\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_cross_border_ratio(x):\n",
    "    \"\"\"Cross-border transactions (if high ratio of cross-border)\"\"\"\n",
    "    condition = (x.cross_border_ratio > 0.17) & (x.transaction_unique_countries >= 3)\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_ecommerce_ratio(x):\n",
    "    \"\"\"High e-commerce transaction ratio (if e-commerce is used for money laundering)\"\"\"\n",
    "    condition = (x.transaction_ecommerce_ratio > 0.98) & (x.transaction_volume_90d > 198059.4) & (x.n_txn_total > 10)\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def ecommerce_occupation_mismatch(x):\n",
    "    \"\"\"High e-commerce for the unemployed\"\"\"\n",
    "    condition = (x.transaction_ecommerce_ratio > 0.98) & (x.occupation_code == 'UNEMPLOYED')\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def unusual_merchant_pattern(x):\n",
    "    \"\"\"Make few transactions across many merchants\"\"\"\n",
    "    condition = (x.transaction_unique_merchants > 40) & (x.n_txn_total < (1.5 * x.transaction_unique_merchants))\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def transaction_same_amount(x):\n",
    "    \"\"\"Predictable amount patterns through e.g., gift cards\"\"\"\n",
    "    condition = (x.transaction_same_amount_frequency_7d > 0.14) & (x.transaction_round_amount_frequency_7d >= 0.5)\n",
    "    return POSITIVE if condition else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75289827",
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_DEFINITIONS = [\n",
    "    (\"High transaction to income ratio\", high_txn_volume_vs_income),\n",
    "    (\"High transaction to occupation median ratio\", high_txn_volume_vs_occupation_median),\n",
    "    (\"High wire transfers\", high_wire),\n",
    "    (\"Frequent wire transfers\", frequent_wire),\n",
    "    (\"Structuring\", structuring),\n",
    "    (\"Short hold time\", short_hold_time),\n",
    "    (\"High cross border ratio\", high_cross_border_ratio),\n",
    "    (\"High ecommerce ratio\", high_ecommerce_ratio),\n",
    "    (\"Ecommerce occupation mismatch\", ecommerce_occupation_mismatch),\n",
    "    (\"Unusual merchant pattern\", unusual_merchant_pattern),\n",
    "    (\"Transaction same amount\", transaction_same_amount),\n",
    "]\n",
    "\n",
    "# Separate convenience lists used throughout the notebook\n",
    "lf_names = [name for name, _ in LF_DEFINITIONS]\n",
    "lfs = [lf for _, lf in LF_DEFINITIONS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e158c6",
   "metadata": {},
   "source": [
    "## Part 2: LF functions Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7fd119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       j Polarity  Coverage  Overlaps  \\\n",
      "high_txn_volume_vs_income              0      [1]  0.006579  0.002193   \n",
      "high_txn_volume_vs_occupation_median   1      [1]  0.003289  0.002193   \n",
      "high_wire                              2      [1]  0.002193  0.002193   \n",
      "frequent_wire                          3      [1]  0.002193  0.002193   \n",
      "structuring                            4      [1]  0.004386  0.000000   \n",
      "short_hold_time                        5      [1]  0.006579  0.000000   \n",
      "high_cross_border_ratio                6       []  0.000000  0.000000   \n",
      "high_ecommerce_ratio                   7       []  0.000000  0.000000   \n",
      "ecommerce_occupation_mismatch          8       []  0.000000  0.000000   \n",
      "unusual_merchant_pattern               9       []  0.000000  0.000000   \n",
      "transaction_same_amount               10       []  0.000000  0.000000   \n",
      "\n",
      "                                      Conflicts  \n",
      "high_txn_volume_vs_income                   0.0  \n",
      "high_txn_volume_vs_occupation_median        0.0  \n",
      "high_wire                                   0.0  \n",
      "frequent_wire                               0.0  \n",
      "structuring                                 0.0  \n",
      "short_hold_time                             0.0  \n",
      "high_cross_border_ratio                     0.0  \n",
      "high_ecommerce_ratio                        0.0  \n",
      "ecommerce_occupation_mismatch               0.0  \n",
      "unusual_merchant_pattern                    0.0  \n",
      "transaction_same_amount                     0.0  \n"
     ]
    }
   ],
   "source": [
    "# read labeled data\n",
    "df_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_labeled.csv'))\n",
    "\n",
    "# Apply LFs to golden_eval\n",
    "L_dev = np.array([[lf(row) for lf in lfs] for _, row in df_labeled.iterrows()])\n",
    "lf_analysis = LFAnalysis(L_dev, lfs=lfs).lf_summary()\n",
    "print(lf_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c78e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       j Polarity  Coverage  Overlaps  \\\n",
      "high_txn_volume_vs_income              0      [1]  0.036695  0.013164   \n",
      "high_txn_volume_vs_occupation_median   1      [1]  0.039627  0.014276   \n",
      "high_wire                              2      [1]  0.004944  0.004005   \n",
      "frequent_wire                          3      [1]  0.005116  0.003794   \n",
      "structuring                            4      [1]  0.010347  0.001686   \n",
      "short_hold_time                        5      [1]  0.004254  0.000019   \n",
      "high_cross_border_ratio                6      [1]  0.001246  0.000134   \n",
      "high_ecommerce_ratio                   7      [1]  0.000211  0.000057   \n",
      "ecommerce_occupation_mismatch          8      [1]  0.000996  0.000134   \n",
      "unusual_merchant_pattern               9       []  0.000000  0.000000   \n",
      "transaction_same_amount               10      [1]  0.000977  0.000038   \n",
      "\n",
      "                                      Conflicts  \n",
      "high_txn_volume_vs_income                   0.0  \n",
      "high_txn_volume_vs_occupation_median        0.0  \n",
      "high_wire                                   0.0  \n",
      "frequent_wire                               0.0  \n",
      "structuring                                 0.0  \n",
      "short_hold_time                             0.0  \n",
      "high_cross_border_ratio                     0.0  \n",
      "high_ecommerce_ratio                        0.0  \n",
      "ecommerce_occupation_mismatch               0.0  \n",
      "unusual_merchant_pattern                    0.0  \n",
      "transaction_same_amount                     0.0  \n"
     ]
    }
   ],
   "source": [
    "# read unlabeled data\n",
    "df_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_unlabeled.csv'))\n",
    "\n",
    "# apply LFs to golden_eval\n",
    "L_dev = np.array([[lf(row) for lf in lfs] for _, row in df_unlabeled.iterrows()])\n",
    "lf_analysis = LFAnalysis(L_dev, lfs=lfs).lf_summary()\n",
    "print(lf_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e46637",
   "metadata": {},
   "source": [
    "## Part 3: Train Snorkel's Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99243169",
   "metadata": {},
   "source": [
    "Snorkel learns to combine LFs into probabilistic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4f13f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52187/52187 [00:09<00:00, 5324.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# read unlabeled data\n",
    "df_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_unlabeled.csv'))\n",
    "\n",
    "# apply labeling functions\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_unlabeled = applier.apply(df_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3978e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|          | 0/50 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.002]\n",
      "INFO:root:[10 epochs]: TRAIN:[loss=0.001]\n",
      "INFO:root:[20 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[30 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[40 epochs]: TRAIN:[loss=0.000]\n",
      "100%|██████████| 50/50 [00:00<00:00, 1014.13epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "# fit Snorkel AI weak supervision model\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_unlabeled, n_epochs=50, log_freq=10, seed=1)\n",
    "\n",
    "# save Snorkel label model\n",
    "label_model_path = os.path.join(CHECKPOINT_DIR, 'snorkel_label_ind.pkl')\n",
    "with open(label_model_path, 'wb') as f:\n",
    "    pickle.dump(label_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51857a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Snorkel label model\n",
    "label_model_path = os.path.join(CHECKPOINT_DIR, 'snorkel_label_ind.pkl')\n",
    "with open(label_model_path, 'rb') as f:\n",
    "    label_model = pickle.load(f)\n",
    "\n",
    "# predict labels for unlabeled data\n",
    "pred_probs_unlabeled = label_model.predict_proba(L_unlabeled)\n",
    "pred_unlabeled = label_model.predict(L_unlabeled)\n",
    "# all predicted 1s are high-risk, while all predicted 0s or abstains are low-risk\n",
    "pred_unlabeled = np.where(pred_unlabeled==1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528694b",
   "metadata": {},
   "source": [
    "## Step 4: Train Final Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb09846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unlabeled data\n",
    "df_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_feature_unlabeled.csv'))\n",
    "\n",
    "# continuous features\n",
    "features_continuous = [c for c in df_unlabeled if c != 'label' and len(df_unlabeled[c].unique()) > 2]\n",
    "df_continuous = df_unlabeled[features_continuous].copy()\n",
    "# fit normalization scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_continuous)\n",
    "# save normalization scaler\n",
    "scaler_path = os.path.join(CHECKPOINT_DIR, 'scaler_ind.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "# transform continuous data (z-scores)\n",
    "X_train = scaler.transform(df_continuous)\n",
    "X_train = pd.DataFrame(X_train, columns=df_continuous.columns)\n",
    "\n",
    "# categorical features\n",
    "features_categorical = [c for c in df_unlabeled if c != 'label' and len(df_unlabeled[c].unique()) <= 2]\n",
    "df_categorical = df_unlabeled[features_categorical].copy()\n",
    "X_train = pd.concat((X_train, df_categorical), axis=1)\n",
    "\n",
    "# fill missing values with zeros\n",
    "X_train = X_train.fillna(value=0)\n",
    "\n",
    "# all features\n",
    "features = features_continuous + features_categorical\n",
    "\n",
    "# labels\n",
    "y_train = pred_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0bd5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost (inherently interpretable with feature importance)\n",
    "classifier = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# save XGBoost model\n",
    "classifier_path = os.path.join(CHECKPOINT_DIR, 'xgboost_ind.pkl')\n",
    "with open(classifier_path, 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f362b",
   "metadata": {},
   "source": [
    "## Step 5: Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1529802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read XGBoost model\n",
    "classifier_path = os.path.join(CHECKPOINT_DIR, 'xgboost_ind.pkl')\n",
    "with open(classifier_path, 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "# read normalization scaler\n",
    "scaler_path = os.path.join(CHECKPOINT_DIR, 'scaler_ind.pkl')\n",
    "with open(scaler_path, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# read labeled data\n",
    "df_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_feature_labeled.csv'))\n",
    "\n",
    "# normalize continuous features (z-scores)\n",
    "df_continuous = df_labeled[features_continuous].copy()\n",
    "X_test = scaler.transform(df_continuous)\n",
    "X_test = pd.DataFrame(X_test, columns=df_continuous.columns)\n",
    "# categorical features\n",
    "df_categorical = df_labeled[features_categorical].copy()\n",
    "X_test = pd.concat((X_test, df_categorical), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f1d4f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9857456140350878\n",
      "Precision (Fraud=1): 0.26666666666666666\n",
      "Recall (Fraud=1): 0.6666666666666666\n",
      "F1 (Fraud=1): 0.38095238095238093\n",
      "AUC: 0.8272626931567328\n",
      "\n",
      "Confusion matrix: [[TN, FP], [FN, TP]]\n",
      "[[895  11]\n",
      " [  2   4]]\n",
      "\n",
      "Full report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99       906\n",
      "         1.0       0.27      0.67      0.38         6\n",
      "\n",
      "    accuracy                           0.99       912\n",
      "   macro avg       0.63      0.83      0.69       912\n",
      "weighted avg       0.99      0.99      0.99       912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = classifier.predict(X_test)\n",
    "target = df_labeled['label']\n",
    "\n",
    "# accuracy\n",
    "a = accuracy_score(target, pred)\n",
    "print(\"Accuracy:\", a)\n",
    "\n",
    "# precision\n",
    "p = precision_score(target, pred)\n",
    "print(\"Precision (Fraud=1):\", p)\n",
    "\n",
    "# recall\n",
    "r = recall_score(target, pred)\n",
    "print(\"Recall (Fraud=1):\", r)\n",
    "\n",
    "# F1 score\n",
    "f1 = f1_score(target, pred)\n",
    "print(\"F1 (Fraud=1):\", f1)\n",
    "\n",
    "# AUC score\n",
    "auc = roc_auc_score(target, pred)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# confusion matrix\n",
    "m = confusion_matrix(target, pred)\n",
    "print(\"\\nConfusion matrix: [[TN, FP], [FN, TP]]\")\n",
    "print(m)\n",
    "\n",
    "cr = classification_report(target, pred)\n",
    "print(\"\\nFull report:\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be4d72",
   "metadata": {},
   "source": [
    "## Step 6: Add SHAP for Local Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c29582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain classification model with SHAP\n",
    "explainer = shap.TreeExplainer(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9d7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_lfs(customer_data):\n",
    "    \"\"\"Which rules fired?\"\"\"\n",
    "    fired_rules = []\n",
    "    for lf, lf_name in zip(lfs, lf_names):\n",
    "        if lf_name != \"\" and lf(customer_data) == POSITIVE:\n",
    "            fired_rules.append(lf_name)\n",
    "    return fired_rules\n",
    "\n",
    "def explain_features(customer_data):\n",
    "    \"\"\"Top features by SHAP\"\"\"\n",
    "    shap_values = explainer.shap_values(customer_data.to_numpy().reshape(1, -1))\n",
    "    # crate tuples of features and scores\n",
    "    feature_impacts = [(f, float(s)) for f, s, in zip(features, shap_values[0])]\n",
    "    # keep features with absolute scores above 0.5\n",
    "    threshold = 0.5\n",
    "    top_features = [(f, float(s)) for f, s in zip(features, shap_values[0]) if abs(s) > threshold]\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8617dd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-risk individuals & LFs\n",
      "----------\n",
      "Sample 31\n",
      "Fired LFs: High transaction to income ratio\n",
      "----------\n",
      "Sample 42\n",
      "Fired LFs: Short hold time\n",
      "----------\n",
      "Sample 60\n",
      "Fired LFs: Short hold time\n",
      "----------\n",
      "Sample 108\n",
      "Fired LFs: \n",
      "----------\n",
      "Sample 179\n",
      "Fired LFs: Short hold time\n",
      "----------\n",
      "Sample 246\n",
      "Fired LFs: High transaction to income ratio\n",
      "----------\n",
      "Sample 389\n",
      "Fired LFs: Short hold time\n",
      "----------\n",
      "Sample 470\n",
      "Fired LFs: Short hold time\n",
      "----------\n",
      "Sample 482\n",
      "Fired LFs: Short hold time\n",
      "----------\n",
      "Sample 513\n",
      "Fired LFs: High transaction to income ratio, High transaction to occupation median ratio\n",
      "----------\n",
      "Sample 615\n",
      "Fired LFs: High transaction to income ratio\n",
      "----------\n",
      "Sample 623\n",
      "Fired LFs: High wire transfers, Frequent wire transfers\n",
      "----------\n",
      "Sample 675\n",
      "Fired LFs: High transaction to occupation median ratio\n",
      "----------\n",
      "Sample 687\n",
      "Fired LFs: High transaction to income ratio, High transaction to occupation median ratio, High wire transfers, Frequent wire transfers\n",
      "----------\n",
      "Sample 741\n",
      "Fired LFs: High transaction to income ratio\n"
     ]
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_labeled.csv'))\n",
    "\n",
    "print(\"High-risk individuals & LFs\")\n",
    "for i, row in df_labeled[pred==1].iterrows():\n",
    "    print('-'*10)\n",
    "    print(f\"Sample {i}\")\n",
    "    lfs_fired = explain_lfs(row)\n",
    "    print('Fired LFs:', ', '.join(lfs_fired))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5a8077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-risk individuals, top features & SHAP values\n",
      "----------\n",
      "Sample 31\n",
      "Top features & SHAP values:\n",
      "income 5.34\n",
      "sum_amt_total 1.69\n",
      "channel_debit_volume -0.57\n",
      "channel_credit_volume 1.24\n",
      "----------\n",
      "Sample 42\n",
      "Top features & SHAP values:\n",
      "channel_credit_volume 2.27\n",
      "median_hold_time_funds 2.27\n",
      "n_txn_total 2.34\n",
      "----------\n",
      "Sample 60\n",
      "Top features & SHAP values:\n",
      "channel_credit_volume 2.41\n",
      "median_hold_time_funds 2.27\n",
      "n_txn_total 2.18\n",
      "----------\n",
      "Sample 108\n",
      "Top features & SHAP values:\n",
      "income 5.02\n",
      "n_txn_total -0.5\n",
      "----------\n",
      "Sample 179\n",
      "Top features & SHAP values:\n",
      "channel_credit_volume 2.49\n",
      "median_hold_time_funds 2.25\n",
      "n_txn_total 2.43\n",
      "----------\n",
      "Sample 246\n",
      "Top features & SHAP values:\n",
      "income 3.7\n",
      "sum_amt_total 2.17\n",
      "channel_credit_volume 0.54\n",
      "----------\n",
      "Sample 389\n",
      "Top features & SHAP values:\n",
      "channel_credit_volume 2.38\n",
      "median_hold_time_funds 2.24\n",
      "n_txn_total 2.42\n",
      "----------\n",
      "Sample 470\n",
      "Top features & SHAP values:\n",
      "channel_credit_volume 2.46\n",
      "median_hold_time_funds 2.25\n",
      "n_txn_total 2.36\n",
      "----------\n",
      "Sample 482\n",
      "Top features & SHAP values:\n",
      "income -0.55\n",
      "channel_credit_volume 2.82\n",
      "median_hold_time_funds 2.81\n",
      "n_txn_total 1.61\n",
      "----------\n",
      "Sample 513\n",
      "Top features & SHAP values:\n",
      "income 4.41\n",
      "sum_amt_total 0.83\n",
      "channel_debit_volume 0.51\n",
      "txn_volume_vs_occupation_median 4.32\n",
      "age -0.59\n",
      "----------\n",
      "Sample 615\n",
      "Top features & SHAP values:\n",
      "income 4.8\n",
      "sum_amt_total 2.07\n",
      "channel_credit_volume 1.46\n",
      "----------\n",
      "Sample 623\n",
      "Top features & SHAP values:\n",
      "median_amt_wire 2.58\n",
      "wire_ratio 3.55\n",
      "income -0.93\n",
      "channel_credit_volume -0.68\n",
      "age 2.46\n",
      "----------\n",
      "Sample 675\n",
      "Top features & SHAP values:\n",
      "income -0.9\n",
      "n_txn_total -0.57\n",
      "txn_volume_vs_occupation_median 8.3\n",
      "age 1.97\n",
      "----------\n",
      "Sample 687\n",
      "Top features & SHAP values:\n",
      "median_amt_wire 1.35\n",
      "wire_ratio 1.7\n",
      "income 2.25\n",
      "txn_volume_vs_occupation_median 4.01\n",
      "age 0.94\n",
      "----------\n",
      "Sample 741\n",
      "Top features & SHAP values:\n",
      "income 5.84\n",
      "sum_amt_total 2.03\n",
      "channel_debit_volume 0.54\n",
      "channel_credit_volume 0.59\n"
     ]
    }
   ],
   "source": [
    "print(\"High-risk individuals, top features & SHAP values\")\n",
    "for i, row in X_test[pred==1].iterrows():\n",
    "    print('-'*10)\n",
    "    print(f\"Sample {i}\")\n",
    "    features_important = explain_features(row)\n",
    "    print(\"Top features & SHAP values:\")\n",
    "    for f, v in features_important:\n",
    "        print(f, round(v, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
