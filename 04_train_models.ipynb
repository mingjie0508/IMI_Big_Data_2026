{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbc1728",
   "metadata": {},
   "source": [
    "# Weak Supervision with Snorkel AI\n",
    "\n",
    "Model individuals and businesses separately.\n",
    "\n",
    "Create labels for unlabeled data with Snorkel AI weak supervision. Train model on newly labeled data. Validate model on labeled data. Generate explanation with Snorkel labeling functions and SHAPLEY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec884138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# Snorkel AI weak supervision\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel\n",
    "# classification model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "# SHAP explainability\n",
    "import shap\n",
    "# save model\n",
    "import pickle\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# directories\n",
    "RAW_DIR = 'data/raw'\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "CHECKPOINT_DIR = 'checkpoint'\n",
    "FINAL_DIR = 'data/final'\n",
    "\n",
    "# weak label constants\n",
    "ABSTAIN = -1  # Uncertain\n",
    "NEGATIVE = 0  # Low risk\n",
    "POSITIVE = 1  # High risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d4a2d",
   "metadata": {},
   "source": [
    "## Model for Businesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32743170",
   "metadata": {},
   "source": [
    "### Part 1: Define Snorkel Labeling Functions LFs (Businesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99690e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def short_hold_time(x):\n",
    "    \"\"\"Short hold time in high risk industries\"\"\"\n",
    "    condition = (\n",
    "        x.industry_code in (\"4561\", \"7761\", \"7499\", \"7214\", \"711\", \"4113\", \"919\", \"7599\", \"7292\", \"7215\") and \n",
    "        x.median_hold_time_funds <= 26.0\n",
    "    )\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_txn_volume_low_sales(x):\n",
    "    \"\"\"High total transaction volume despite low sales in high risk industries\"\"\"\n",
    "    condition = (\n",
    "        x.industry_code in (\"4561\", \"7761\", \"7499\", \"7214\", \"711\", \"4113\", \"919\", \"7599\", \"7292\", \"7215\") and \n",
    "        x.sales <= 14515.0 and \n",
    "        x.sum_amt_total >= 209466.1\n",
    "    )\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def low_channel_diversification(x):\n",
    "    \"\"\"High transaction count with low channel diversification in high risk industries\"\"\"\n",
    "    condition = (\n",
    "        x.industry_code in (\"4561\", \"7761\", \"7499\", \"7214\", \"711\", \"4113\", \"919\", \"7599\", \"7292\", \"7215\") and \n",
    "        x.max_channel_share <= 0.820 and \n",
    "        x.n_txn_total >= 70.0\n",
    "    )\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def transaction_same_amount(x):\n",
    "    \"\"\"Suspicious transaction patterns (repetitive amounts or high e-commerce ratio) in high risk industries\"\"\"\n",
    "    condition = (\n",
    "        x.industry_code in (\"4561\", \"7761\", \"7499\", \"7214\", \"711\", \"4113\", \"919\", \"7599\", \"7292\", \"7215\") and \n",
    "        (x.transaction_same_amount_frequency_90d >= 0.013 or x.transaction_ecommerce_ratio >= 0.087)\n",
    "    )\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def infrequent_transaction(x):\n",
    "    \"\"\"Low transaction count (likely inactive/low activity)\"\"\"\n",
    "    condition = x.n_txn_total <= 8.0\n",
    "    return NEGATIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def long_hold_time(x):\n",
    "    \"\"\"Funds held for a long period (slow movement of money)\"\"\"\n",
    "    condition = x.median_hold_time_funds > 172.1\n",
    "    return NEGATIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def low_debit_transfers(x):\n",
    "    \"\"\"Low debit activity (low money going out)\"\"\"\n",
    "    condition = x.channel_debit_volume <= 2690.3\n",
    "    return NEGATIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def no_recent_transaction(x):\n",
    "    \"\"\"No recent transaction (inactive account)\"\"\"\n",
    "    condition = x.days_since_last_transaction > 19.0\n",
    "    return NEGATIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def low_var_transaction(x):\n",
    "    \"\"\"Low variability in transaction amounts across specific channels (consistent/predictable usage)\"\"\"\n",
    "    condition = (x.cv_amt_emt <= 1.087 and x.cv_amt_eft <= 1.8 and x.std_amt_card <= 152.6)\n",
    "    return NEGATIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_var_wire(x):\n",
    "    \"\"\"Use of wire transfers in high risk industries\"\"\"\n",
    "    condition = (\n",
    "        x.industry_code in (\"4561\", \"7761\", \"7499\", \"7214\", \"711\", \"4113\", \"919\", \"7599\", \"7292\", \"7215\") and \n",
    "        x.std_amt_wire > 73018.4\n",
    "    )\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_sale_high_debit(x):\n",
    "    \"\"\"High sales paired with high debit activity in high risk industries\"\"\"\n",
    "    condition = (\n",
    "        x.industry_code in (\"4561\", \"7761\", \"7499\", \"7214\", \"711\", \"4113\", \"919\", \"7599\", \"7292\", \"7215\") and \n",
    "        x.sales >= 98725.8 and x.channel_debit_volume >= 126121.7\n",
    "    )\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def frequent_transaction_short_hold_time(x):\n",
    "    \"\"\"High transaction frequency with short hold times in high risk industries\"\"\"\n",
    "    condition = (\n",
    "        x.industry_code in (\"4561\", \"7761\", \"7499\", \"7214\", \"711\", \"4113\", \"919\", \"7599\", \"7292\", \"7215\") and \n",
    "        x.transaction_frequency_7d >= 6.0 and \n",
    "        x.median_hold_time_funds <= 33.356\n",
    "    )\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_var_emt(x):\n",
    "    \"\"\"High variability in EMT amounts combined with other factors (variability in cards/EFT and high risk industries)\"\"\"\n",
    "    condition = (\n",
    "        x.industry_code in (\"4561\", \"7761\", \"7499\", \"7214\", \"711\", \"4113\", \"919\", \"7599\", \"7292\", \"7215\") and \n",
    "        (x.cv_amt_emt >= 1.538 or x.std_amt_card >= 817.522 or x.cv_amt_eft >= 2.956)\n",
    "    )\n",
    "    return POSITIVE if condition else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bea455",
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_DEFINITIONS = [\n",
    "    (\"Short hold time\", short_hold_time),\n",
    "    (\"High transaction volume and low sales\", high_txn_volume_low_sales),\n",
    "    (\"Low channel diversification\", low_channel_diversification),\n",
    "    (\"Many transactions of the same amount\", transaction_same_amount),\n",
    "    (\"Infrequent transaction\", infrequent_transaction),\n",
    "    (\"Long hold time\", long_hold_time),\n",
    "    (\"Low debit transfers\", low_debit_transfers),\n",
    "    (\"No recent transaction\", no_recent_transaction),\n",
    "    (\"Low variability in transaction amounts\", low_var_transaction),\n",
    "    (\"High variability in wire transfers\", high_var_wire),\n",
    "    (\"High sale and high debit transfers\", high_sale_high_debit),\n",
    "    (\"Frequent transaction and short hold time\", frequent_transaction_short_hold_time),\n",
    "    (\"High variability in EMT transfers\", high_var_emt)\n",
    "]\n",
    "\n",
    "# Separate convenience lists used throughout the notebook\n",
    "lf_names = [name for name, _ in LF_DEFINITIONS]\n",
    "lfs = [lf for _, lf in LF_DEFINITIONS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a73f5",
   "metadata": {},
   "source": [
    "### Part 2: LF functions Sanity Check (Businesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e827c6",
   "metadata": {},
   "source": [
    "Check the percentage of samples that satisfy each LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bb3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       j Polarity  Coverage  Overlaps  \\\n",
      "short_hold_time                        0      [1]  0.079545  0.079545   \n",
      "high_txn_volume_low_sales              1       []  0.000000  0.000000   \n",
      "low_channel_diversification            2      [1]  0.022727  0.022727   \n",
      "transaction_same_amount                3      [1]  0.045455  0.045455   \n",
      "infrequent_transaction                 4      [0]  0.204545  0.204545   \n",
      "long_hold_time                         5      [0]  0.204545  0.136364   \n",
      "low_debit_transfers                    6      [0]  0.204545  0.181818   \n",
      "no_recent_transaction                  7      [0]  0.136364  0.136364   \n",
      "low_var_transaction                    8      [0]  0.454545  0.295455   \n",
      "high_var_wire                          9       []  0.000000  0.000000   \n",
      "high_sale_high_debit                  10      [1]  0.011364  0.011364   \n",
      "frequent_transaction_short_hold_time  11      [1]  0.011364  0.011364   \n",
      "high_var_emt                          12      [1]  0.034091  0.011364   \n",
      "\n",
      "                                      Conflicts  \n",
      "short_hold_time                        0.056818  \n",
      "high_txn_volume_low_sales              0.000000  \n",
      "low_channel_diversification            0.000000  \n",
      "transaction_same_amount                0.011364  \n",
      "infrequent_transaction                 0.034091  \n",
      "long_hold_time                         0.000000  \n",
      "low_debit_transfers                    0.022727  \n",
      "no_recent_transaction                  0.022727  \n",
      "low_var_transaction                    0.045455  \n",
      "high_var_wire                          0.000000  \n",
      "high_sale_high_debit                   0.011364  \n",
      "frequent_transaction_short_hold_time   0.011364  \n",
      "high_var_emt                           0.000000  \n"
     ]
    }
   ],
   "source": [
    "# read labeled data\n",
    "df_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'business_labeled.csv'))\n",
    "\n",
    "# Apply LFs to golden_eval\n",
    "L_dev = np.array([[lf(row) for lf in lfs] for _, row in df_labeled.iterrows()])\n",
    "lf_analysis = LFAnalysis(L_dev, lfs=lfs).lf_summary()\n",
    "print(lf_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc713faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       j Polarity  Coverage  Overlaps  \\\n",
      "short_hold_time                        0      [1]  0.062994  0.062021   \n",
      "high_txn_volume_low_sales              1      [1]  0.022133  0.020795   \n",
      "low_channel_diversification            2      [1]  0.020066  0.018728   \n",
      "transaction_same_amount                3      [1]  0.034051  0.028943   \n",
      "infrequent_transaction                 4      [0]  0.190441  0.188009   \n",
      "long_hold_time                         5      [0]  0.179861  0.137906   \n",
      "low_debit_transfers                    6      [0]  0.149459  0.141919   \n",
      "no_recent_transaction                  7      [0]  0.093640  0.092302   \n",
      "low_var_transaction                    8      [0]  0.361182  0.236045   \n",
      "high_var_wire                          9      [1]  0.001459  0.001338   \n",
      "high_sale_high_debit                  10      [1]  0.010094  0.009121   \n",
      "frequent_transaction_short_hold_time  11      [1]  0.031254  0.031011   \n",
      "high_var_emt                          12      [1]  0.040739  0.032105   \n",
      "\n",
      "                                      Conflicts  \n",
      "short_hold_time                        0.030159  \n",
      "high_txn_volume_low_sales              0.006445  \n",
      "low_channel_diversification            0.000730  \n",
      "transaction_same_amount                0.005594  \n",
      "infrequent_transaction                 0.026389  \n",
      "long_hold_time                         0.004864  \n",
      "low_debit_transfers                    0.021525  \n",
      "no_recent_transaction                  0.015080  \n",
      "low_var_transaction                    0.036726  \n",
      "high_var_wire                          0.000486  \n",
      "high_sale_high_debit                   0.001946  \n",
      "frequent_transaction_short_hold_time   0.002432  \n",
      "high_var_emt                           0.003040  \n"
     ]
    }
   ],
   "source": [
    "# read unlabeled data\n",
    "df_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'business_unlabeled.csv'))\n",
    "\n",
    "# apply LFs to golden_eval\n",
    "L_dev = np.array([[lf(row) for lf in lfs] for _, row in df_unlabeled.iterrows()])\n",
    "lf_analysis = LFAnalysis(L_dev, lfs=lfs).lf_summary()\n",
    "print(lf_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bfb05",
   "metadata": {},
   "source": [
    "### Part 3: Train Snorkel's Generative Model (Businesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc59b5",
   "metadata": {},
   "source": [
    "Snorkel learns to combine LFs into probabilistic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d18bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8223/8223 [00:01<00:00, 7689.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# read unlabeled data\n",
    "df_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'business_unlabeled.csv'))\n",
    "\n",
    "# apply labeling functions to unlabeled data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_unlabeled = applier.apply(df_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae3ddd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Estimating \\mu...\n",
      "  0%|          | 0/50 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.298]\n",
      "INFO:root:[10 epochs]: TRAIN:[loss=0.153]\n",
      "INFO:root:[20 epochs]: TRAIN:[loss=0.037]\n",
      "INFO:root:[30 epochs]: TRAIN:[loss=0.046]\n",
      "INFO:root:[40 epochs]: TRAIN:[loss=0.028]\n",
      "100%|██████████| 50/50 [00:00<00:00, 924.06epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "# fit Snorkel AI weak supervision model on unlabeled data\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_unlabeled, n_epochs=50, log_freq=10, seed=1)\n",
    "\n",
    "# save Snorkel model\n",
    "label_model_path = os.path.join(CHECKPOINT_DIR, 'snorkel_label_bsn.pkl')\n",
    "with open(label_model_path, 'wb') as f:\n",
    "    pickle.dump(label_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af65f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Snorkel model\n",
    "label_model_path = os.path.join(CHECKPOINT_DIR, 'snorkel_label_bsn.pkl')\n",
    "with open(label_model_path, 'rb') as f:\n",
    "    label_model = pickle.load(f)\n",
    "\n",
    "# predict labels for unlabeled data\n",
    "pred_probs_unlabeled = label_model.predict_proba(L_unlabeled)\n",
    "pred_unlabeled = label_model.predict(L_unlabeled)\n",
    "# all predicted 1s are high-risk, while all predicted 0s or abstains are low-risk\n",
    "pred_unlabeled = np.where(pred_unlabeled==1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56171452",
   "metadata": {},
   "source": [
    "### Step 4: Train Final Classification Model (Businesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db21ae",
   "metadata": {},
   "source": [
    "Train classification model on newly labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093a9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unlabeled data\n",
    "df_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'business_feature_unlabeled.csv'))\n",
    "\n",
    "# continuous features\n",
    "features_continuous = [c for c in df_unlabeled if c != 'label' and len(df_unlabeled[c].unique()) > 2]\n",
    "df_continuous = df_unlabeled[features_continuous].copy()\n",
    "# fit normalization scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_continuous)\n",
    "# save normalization scaler\n",
    "scaler_path = os.path.join(CHECKPOINT_DIR, 'scaler_bsn.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "# transform continuous data (z-scores)\n",
    "X_train = scaler.transform(df_continuous)\n",
    "X_train = pd.DataFrame(X_train, columns=df_continuous.columns)\n",
    "\n",
    "# categorical features\n",
    "features_categorical = [c for c in df_unlabeled if c != 'label' and len(df_unlabeled[c].unique()) <= 2]\n",
    "df_categorical = df_unlabeled[features_categorical].copy()\n",
    "X_train = pd.concat((X_train, df_categorical), axis=1)\n",
    "\n",
    "# fill missing values with zeros\n",
    "X_train = X_train.fillna(value=0)\n",
    "\n",
    "# all features\n",
    "features = features_continuous + features_categorical\n",
    "\n",
    "# labels\n",
    "y_train = pred_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f9865f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost (inherently interpretable with feature importance)\n",
    "classifier = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    random_state=1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# save XGBoost model\n",
    "classifier_path = os.path.join(CHECKPOINT_DIR, 'xgboost_bsn.pkl')\n",
    "with open(classifier_path, 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18df5e",
   "metadata": {},
   "source": [
    "### Step 5: Model Performance (Businesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be70ab2",
   "metadata": {},
   "source": [
    "Evaluate performance of classification model on labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d4c50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read XGBoost model\n",
    "classifier_path = os.path.join(CHECKPOINT_DIR, 'xgboost_bsn.pkl')\n",
    "with open(classifier_path, 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "# read normalization scaler\n",
    "scaler_path = os.path.join(CHECKPOINT_DIR, 'scaler_bsn.pkl')\n",
    "with open(scaler_path, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# read labeled data\n",
    "df_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'business_feature_labeled.csv'))\n",
    "\n",
    "# normalize continuous features (z-scores)\n",
    "df_continuous = df_labeled[features_continuous].copy()\n",
    "X_test = scaler.transform(df_continuous)\n",
    "X_test = pd.DataFrame(X_test, columns=df_continuous.columns)\n",
    "# categorical features\n",
    "df_categorical = df_labeled[features_categorical].copy()\n",
    "X_test = pd.concat((X_test, df_categorical), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acceac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9545454545454546\n",
      "Precision (Fraud=1): 0.5\n",
      "Recall (Fraud=1): 1.0\n",
      "F1 (Fraud=1): 0.6666666666666666\n",
      "AUC: 0.9761904761904762\n",
      "\n",
      "Confusion matrix: [[TN, FP], [FN, TP]]\n",
      "[[80  4]\n",
      " [ 0  4]]\n",
      "\n",
      "Full report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.98        84\n",
      "         1.0       0.50      1.00      0.67         4\n",
      "\n",
      "    accuracy                           0.95        88\n",
      "   macro avg       0.75      0.98      0.82        88\n",
      "weighted avg       0.98      0.95      0.96        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = classifier.predict(X_test)\n",
    "target = df_labeled['label']\n",
    "\n",
    "# accuracy\n",
    "a = accuracy_score(target, pred)\n",
    "print(\"Accuracy:\", a)\n",
    "\n",
    "# precision\n",
    "p = precision_score(target, pred)\n",
    "print(\"Precision (Fraud=1):\", p)\n",
    "\n",
    "# recall\n",
    "r = recall_score(target, pred)\n",
    "print(\"Recall (Fraud=1):\", r)\n",
    "\n",
    "# F1 score\n",
    "f1 = f1_score(target, pred)\n",
    "print(\"F1 (Fraud=1):\", f1)\n",
    "\n",
    "# AUC score\n",
    "auc = roc_auc_score(target, pred)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# confusion matrix\n",
    "m = confusion_matrix(target, pred)\n",
    "print(\"\\nConfusion matrix: [[TN, FP], [FN, TP]]\")\n",
    "print(m)\n",
    "\n",
    "cr = classification_report(target, pred)\n",
    "print(\"\\nFull report:\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41fe16a",
   "metadata": {},
   "source": [
    "### Step 6: Add SHAP for Local Interpretability (Businesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddaa24",
   "metadata": {},
   "source": [
    "Generate explanation for each client with LFs and SHAPLEY\n",
    "- LFs that are satisfied\n",
    "- Top features based on SHAPLEY values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b97029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain classification model with SHAP\n",
    "explainer = shap.TreeExplainer(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18177d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_lfs(customer_data):\n",
    "    \"\"\"Which rules fired?\"\"\"\n",
    "    fired_rules = []\n",
    "    for lf, lf_name in zip(lfs, lf_names):\n",
    "        if lf_name != \"\" and lf(customer_data) == POSITIVE:\n",
    "            fired_rules.append(lf_name)\n",
    "    return fired_rules\n",
    "\n",
    "def explain_features(transformed_data, threshold=0.5):\n",
    "    \"\"\"Top features based on SHAPLEY values\"\"\"\n",
    "    shap_values = explainer.shap_values(transformed_data.to_numpy().reshape(1, -1))\n",
    "    # crate tuples of features and scores\n",
    "    feature_impacts = [(f, float(s)) for f, s, in zip(features, shap_values[0])]\n",
    "    # keep features with absolute scores above 0.5\n",
    "    top_features = [(f, float(s)) for f, s in zip(features, shap_values[0]) if abs(s) > threshold]\n",
    "    return top_features\n",
    "\n",
    "def explain(customer_data, transformed_data, pred, threshold=0.5):\n",
    "    \"\"\"LFs that are satisfied anf top features based on SHAPLEY values\"\"\"\n",
    "    # lfs\n",
    "    lfs_fired = explain_lfs(customer_data)\n",
    "    # features with SHAP\n",
    "    features_important = explain_features(transformed_data, threshold)\n",
    "    features_description = []\n",
    "    # generate description for top features:\n",
    "    # e.g. high, low, satisfied, not satisfied\n",
    "    for f, v in features_important:\n",
    "        if v >= 0 and p == 1 or v < 0 and p == 0:\n",
    "            if v >= 0:\n",
    "                d = \"satisfied\" if f.startswith(('is_', 'has_')) else \"high\"\n",
    "            else:\n",
    "                d = \"not satisfied\" if f.startswith(('is_', 'has_')) else \"low\"\n",
    "            features_description.append((d, f, v))\n",
    "    return lfs_fired, features_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6bda899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted high-risk businesses\n",
      "----------\n",
      "Sample 0\n",
      "Fired LFs:\n",
      "- Short hold time\n",
      "Top features & SHAP values:\n",
      "- high n_txn_total 1.04\n",
      "- high sum_amt_total 1.01\n",
      "- high median_hold_time_funds 0.64\n",
      "- satisfied is_industry_code_7215 4.62\n",
      "----------\n",
      "Sample 11\n",
      "Fired LFs:\n",
      "- Short hold time\n",
      "- Many transactions of the same amount\n",
      "Top features & SHAP values:\n",
      "- high sum_amt_card 0.73\n",
      "- satisfied is_industry_code_7761 4.35\n",
      "----------\n",
      "Sample 36\n",
      "Fired LFs:\n",
      "- High variability in EMT transfers\n",
      "Top features & SHAP values:\n",
      "- high cv_amt_eft 0.82\n",
      "- satisfied is_industry_code_7599 3.83\n",
      "----------\n",
      "Sample 40\n",
      "Fired LFs:\n",
      "Top features & SHAP values:\n",
      "- high sum_amt_card 0.62\n",
      "- satisfied is_industry_code_7761 4.45\n",
      "----------\n",
      "Sample 44\n",
      "Fired LFs:\n",
      "- Short hold time\n",
      "- High sale and high debit transfers\n",
      "- Frequent transaction and short hold time\n",
      "Top features & SHAP values:\n",
      "- high n_txn_total 1.12\n",
      "- high sum_amt_total 1.01\n",
      "- high median_hold_time_funds 0.66\n",
      "- satisfied is_industry_code_7215 4.62\n",
      "----------\n",
      "Sample 55\n",
      "Fired LFs:\n",
      "- Low channel diversification\n",
      "- Many transactions of the same amount\n",
      "Top features & SHAP values:\n",
      "- high n_txn_total 0.76\n",
      "- high sum_amt_emt 0.89\n",
      "- satisfied is_industry_code_4561 4.1\n",
      "----------\n",
      "Sample 56\n",
      "Fired LFs:\n",
      "- Short hold time\n",
      "- Low channel diversification\n",
      "- Many transactions of the same amount\n",
      "- High variability in EMT transfers\n",
      "Top features & SHAP values:\n",
      "- high n_txn_total 1.03\n",
      "- high median_hold_time_funds 0.59\n",
      "- satisfied is_industry_code_7215 4.64\n",
      "----------\n",
      "Sample 57\n",
      "Fired LFs:\n",
      "Top features & SHAP values:\n",
      "- high n_txn_total 0.95\n",
      "- high sum_amt_total 0.91\n",
      "- satisfied is_industry_code_7215 4.38\n"
     ]
    }
   ],
   "source": [
    "# read labeled data\n",
    "df_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'business_labeled.csv'))\n",
    "\n",
    "print(\"Predicted high-risk businesses\")\n",
    "for (i, customer_data), (i, transformed_data), p in zip(df_labeled.iterrows(), X_test.iterrows(), pred):\n",
    "    if p == 1:\n",
    "        print('-'*10)\n",
    "        print(f\"Sample {i}\")\n",
    "        lfs_fired, features_description = explain(customer_data, transformed_data, p)\n",
    "        print('Fired LFs:')\n",
    "        for l in lfs_fired:\n",
    "            print(\"-\", l)\n",
    "        print(\"Top features & SHAP values:\")\n",
    "        for d, f, v in features_description:\n",
    "            print(\"-\", d, f, round(v, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45dd9d8",
   "metadata": {},
   "source": [
    "### Step 7: Get final output (Businesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c9b7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master dataframes (businesses)\n",
    "df_bsn_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'business_labeled.csv'))\n",
    "df_bsn_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'business_unlabeled.csv'))\n",
    "# feature selected dataframes (businesses)\n",
    "df_bsn_feat_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'business_feature_labeled.csv'))\n",
    "df_bsn_feat_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'business_feature_unlabeled.csv'))\n",
    "# all data (businesses)\n",
    "df_bsn = pd.concat((df_bsn_labeled, df_bsn_unlabeled))\n",
    "df_bsn_feat = pd.concat((df_bsn_feat_labeled, df_bsn_feat_unlabeled), ignore_index=True)\n",
    "\n",
    "# normalize continuous features (z-scores)\n",
    "df_continuous = df_bsn_feat[features_continuous].copy()\n",
    "df_bsn_transformed = scaler.transform(df_continuous)\n",
    "df_bsn_transformed = pd.DataFrame(df_bsn_transformed, columns=df_continuous.columns)\n",
    "# categorical features\n",
    "df_categorical = df_bsn_feat[features_categorical].copy()\n",
    "df_bsn_transformed = pd.concat((df_bsn_transformed, df_categorical), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1000da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted labels\n",
    "pred = classifier.predict(df_bsn_transformed)\n",
    "\n",
    "# get explanation for each client\n",
    "explanation_list = []\n",
    "for (i, customer_data), (i, transformed_data), p in zip(df_bsn.iterrows(), df_bsn_transformed.iterrows(), pred):\n",
    "    lfs_fired, features_description = explain(customer_data, transformed_data, p)\n",
    "    explanation = \"\"\n",
    "    if len(lfs_fired) > 0:\n",
    "        explanation += ', '.join(lfs_fired) + '. '\n",
    "    if len(features_description) > 0:\n",
    "        explanation += \"Top features: \" + ', '.join([d+' '+f for d, f, _ in features_description])\n",
    "    explanation_list.append(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2d26e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame({\n",
    "    'customer_id': df_bsn['customer_id'],\n",
    "    'label': df_bsn['label'],\n",
    "    'prediction': pred,\n",
    "    'explanation': explanation_list\n",
    "})\n",
    "df_final.to_csv(os.path.join(FINAL_DIR, 'business_output.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f0ffb",
   "metadata": {},
   "source": [
    "## Model for Individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd582a8c",
   "metadata": {},
   "source": [
    "### Part 1: Define Snorkel Labeling Functions LFs (Individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd61387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def high_txn_volume_vs_income(x):\n",
    "    \"\"\"High transaction volume relative to income (suspicious if transaction volume is much higher than income)\"\"\"\n",
    "    condition = (x.txn_volume_vs_income > 2477.8) & ((x.age <= 25) | (x.income <= 40000))\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_txn_volume_vs_occupation_median(x):\n",
    "    \"\"\"Transaction volume vs occupation median (if transaction volume is high compared to typical for the occupation)\"\"\"\n",
    "    condition = (x.txn_volume_vs_occupation_median > 1562.0) & ((x.age <= 25) | (x.income <= 40000))\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_wire(x):\n",
    "    \"\"\"High wire transfer amounts if income or age is low (could be mule)\"\"\"\n",
    "    condition = (x.median_amt_wire > 6817.2) & ((x.age <= 25) | (x.income <= 40000))\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def frequent_wire(x):\n",
    "    \"\"\"Frequent wire transfer amounts if income or age is low\"\"\"\n",
    "    condition = (x.wire_ratio > 0.47) & ((x.age <= 25) | (x.income <= 40000))\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def structuring(x):\n",
    "    \"\"\"Structuring (frequent transactions just below the reporting threshold)\"\"\"\n",
    "    condition = (x.count_txn_below_threshold_frequency > 0.009) & (x.n_txn_total > 5)\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def short_hold_time(x):\n",
    "    \"\"\"Rapid movement of funds (short hold time)\"\"\"\n",
    "    condition = (x.n_txn_total > 50) & (x.median_hold_time_funds <= 0.11)\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_cross_border_ratio(x):\n",
    "    \"\"\"Cross-border transactions (if high ratio of cross-border)\"\"\"\n",
    "    condition = (x.cross_border_ratio > 0.17) & (x.transaction_unique_countries >= 3)\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def high_ecommerce_ratio(x):\n",
    "    \"\"\"High e-commerce transaction ratio (if e-commerce is used for money laundering)\"\"\"\n",
    "    condition = (x.transaction_ecommerce_ratio > 0.98) & (x.transaction_volume_90d > 198059.4) & (x.n_txn_total > 10)\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def ecommerce_occupation_mismatch(x):\n",
    "    \"\"\"High e-commerce for the unemployed\"\"\"\n",
    "    condition = (x.transaction_ecommerce_ratio > 0.98) & (x.occupation_code == 'UNEMPLOYED')\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def unusual_merchant_pattern(x):\n",
    "    \"\"\"Make few transactions across many merchants\"\"\"\n",
    "    condition = (x.transaction_unique_merchants > 40) & (x.n_txn_total < (1.5 * x.transaction_unique_merchants))\n",
    "    return POSITIVE if condition else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def transaction_same_amount(x):\n",
    "    \"\"\"Predictable amount patterns through e.g., gift cards\"\"\"\n",
    "    condition = (x.transaction_same_amount_frequency_7d > 0.14) & (x.transaction_round_amount_frequency_7d >= 0.5)\n",
    "    return POSITIVE if condition else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75289827",
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_DEFINITIONS = [\n",
    "    (\"High transaction to income ratio\", high_txn_volume_vs_income),\n",
    "    (\"High transaction to occupation median ratio\", high_txn_volume_vs_occupation_median),\n",
    "    (\"High wire transfers\", high_wire),\n",
    "    (\"Frequent wire transfers\", frequent_wire),\n",
    "    (\"Structuring\", structuring),\n",
    "    (\"Short hold time\", short_hold_time),\n",
    "    (\"High cross border ratio\", high_cross_border_ratio),\n",
    "    (\"High ecommerce ratio\", high_ecommerce_ratio),\n",
    "    (\"Ecommerce occupation mismatch\", ecommerce_occupation_mismatch),\n",
    "    (\"Unusual merchant pattern\", unusual_merchant_pattern),\n",
    "    (\"Transaction same amount\", transaction_same_amount),\n",
    "]\n",
    "\n",
    "# Separate convenience lists used throughout the notebook\n",
    "lf_names = [name for name, _ in LF_DEFINITIONS]\n",
    "lfs = [lf for _, lf in LF_DEFINITIONS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e158c6",
   "metadata": {},
   "source": [
    "### Part 2: LF functions Sanity Check (Individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd5110",
   "metadata": {},
   "source": [
    "Check the percentage of samples that satisfy each LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7fd119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       j Polarity  Coverage  Overlaps  \\\n",
      "high_txn_volume_vs_income              0      [1]  0.006579  0.002193   \n",
      "high_txn_volume_vs_occupation_median   1      [1]  0.003289  0.002193   \n",
      "high_wire                              2      [1]  0.002193  0.002193   \n",
      "frequent_wire                          3      [1]  0.002193  0.002193   \n",
      "structuring                            4      [1]  0.004386  0.000000   \n",
      "short_hold_time                        5      [1]  0.006579  0.000000   \n",
      "high_cross_border_ratio                6       []  0.000000  0.000000   \n",
      "high_ecommerce_ratio                   7       []  0.000000  0.000000   \n",
      "ecommerce_occupation_mismatch          8       []  0.000000  0.000000   \n",
      "unusual_merchant_pattern               9       []  0.000000  0.000000   \n",
      "transaction_same_amount               10       []  0.000000  0.000000   \n",
      "\n",
      "                                      Conflicts  \n",
      "high_txn_volume_vs_income                   0.0  \n",
      "high_txn_volume_vs_occupation_median        0.0  \n",
      "high_wire                                   0.0  \n",
      "frequent_wire                               0.0  \n",
      "structuring                                 0.0  \n",
      "short_hold_time                             0.0  \n",
      "high_cross_border_ratio                     0.0  \n",
      "high_ecommerce_ratio                        0.0  \n",
      "ecommerce_occupation_mismatch               0.0  \n",
      "unusual_merchant_pattern                    0.0  \n",
      "transaction_same_amount                     0.0  \n"
     ]
    }
   ],
   "source": [
    "# read labeled data\n",
    "df_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_labeled.csv'))\n",
    "\n",
    "# Apply LFs to golden_eval\n",
    "L_dev = np.array([[lf(row) for lf in lfs] for _, row in df_labeled.iterrows()])\n",
    "lf_analysis = LFAnalysis(L_dev, lfs=lfs).lf_summary()\n",
    "print(lf_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c78e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       j Polarity  Coverage  Overlaps  \\\n",
      "high_txn_volume_vs_income              0      [1]  0.036695  0.013164   \n",
      "high_txn_volume_vs_occupation_median   1      [1]  0.039627  0.014276   \n",
      "high_wire                              2      [1]  0.004944  0.004005   \n",
      "frequent_wire                          3      [1]  0.005116  0.003794   \n",
      "structuring                            4      [1]  0.010347  0.001686   \n",
      "short_hold_time                        5      [1]  0.004254  0.000019   \n",
      "high_cross_border_ratio                6      [1]  0.001246  0.000134   \n",
      "high_ecommerce_ratio                   7      [1]  0.000211  0.000057   \n",
      "ecommerce_occupation_mismatch          8      [1]  0.000996  0.000134   \n",
      "unusual_merchant_pattern               9       []  0.000000  0.000000   \n",
      "transaction_same_amount               10      [1]  0.000977  0.000038   \n",
      "\n",
      "                                      Conflicts  \n",
      "high_txn_volume_vs_income                   0.0  \n",
      "high_txn_volume_vs_occupation_median        0.0  \n",
      "high_wire                                   0.0  \n",
      "frequent_wire                               0.0  \n",
      "structuring                                 0.0  \n",
      "short_hold_time                             0.0  \n",
      "high_cross_border_ratio                     0.0  \n",
      "high_ecommerce_ratio                        0.0  \n",
      "ecommerce_occupation_mismatch               0.0  \n",
      "unusual_merchant_pattern                    0.0  \n",
      "transaction_same_amount                     0.0  \n"
     ]
    }
   ],
   "source": [
    "# read unlabeled data\n",
    "df_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_unlabeled.csv'))\n",
    "\n",
    "# apply LFs to golden_eval\n",
    "L_dev = np.array([[lf(row) for lf in lfs] for _, row in df_unlabeled.iterrows()])\n",
    "lf_analysis = LFAnalysis(L_dev, lfs=lfs).lf_summary()\n",
    "print(lf_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e46637",
   "metadata": {},
   "source": [
    "### Part 3: Train Snorkel's Generative Model (Individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99243169",
   "metadata": {},
   "source": [
    "Snorkel learns to combine LFs into probabilistic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4f13f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52187/52187 [00:09<00:00, 5350.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# read unlabeled data\n",
    "df_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_unlabeled.csv'))\n",
    "\n",
    "# apply labeling functions to unlabeled data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_unlabeled = applier.apply(df_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3978e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|          | 0/50 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.002]\n",
      "INFO:root:[10 epochs]: TRAIN:[loss=0.001]\n",
      "INFO:root:[20 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[30 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[40 epochs]: TRAIN:[loss=0.000]\n",
      "100%|██████████| 50/50 [00:00<00:00, 1014.13epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "# fit Snorkel AI weak supervision model on unlabeled data\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_unlabeled, n_epochs=50, log_freq=10, seed=1)\n",
    "\n",
    "# save Snorkel model\n",
    "label_model_path = os.path.join(CHECKPOINT_DIR, 'snorkel_label_ind.pkl')\n",
    "with open(label_model_path, 'wb') as f:\n",
    "    pickle.dump(label_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51857a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Snorkel model\n",
    "label_model_path = os.path.join(CHECKPOINT_DIR, 'snorkel_label_ind.pkl')\n",
    "with open(label_model_path, 'rb') as f:\n",
    "    label_model = pickle.load(f)\n",
    "\n",
    "# predict labels for unlabeled data\n",
    "pred_probs_unlabeled = label_model.predict_proba(L_unlabeled)\n",
    "pred_unlabeled = label_model.predict(L_unlabeled)\n",
    "# all predicted 1s are high-risk, while all predicted 0s or abstains are low-risk\n",
    "pred_unlabeled = np.where(pred_unlabeled==1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528694b",
   "metadata": {},
   "source": [
    "### Step 4: Train Final Classification Model (Individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d60d36",
   "metadata": {},
   "source": [
    "Train classification model on newly labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb09846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unlabeled data\n",
    "df_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_feature_unlabeled.csv'))\n",
    "\n",
    "# continuous features\n",
    "features_continuous = [c for c in df_unlabeled if c != 'label' and len(df_unlabeled[c].unique()) > 2]\n",
    "df_continuous = df_unlabeled[features_continuous].copy()\n",
    "# fit normalization scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_continuous)\n",
    "# save normalization scaler\n",
    "scaler_path = os.path.join(CHECKPOINT_DIR, 'scaler_ind.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "# transform continuous data (z-scores)\n",
    "X_train = scaler.transform(df_continuous)\n",
    "X_train = pd.DataFrame(X_train, columns=df_continuous.columns)\n",
    "\n",
    "# categorical features\n",
    "features_categorical = [c for c in df_unlabeled if c != 'label' and len(df_unlabeled[c].unique()) <= 2]\n",
    "df_categorical = df_unlabeled[features_categorical].copy()\n",
    "X_train = pd.concat((X_train, df_categorical), axis=1)\n",
    "\n",
    "# fill missing values with zeros\n",
    "X_train = X_train.fillna(value=0)\n",
    "\n",
    "# all features\n",
    "features = features_continuous + features_categorical\n",
    "\n",
    "# labels\n",
    "y_train = pred_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0bd5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost (inherently interpretable with feature importance)\n",
    "classifier = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# save XGBoost model\n",
    "classifier_path = os.path.join(CHECKPOINT_DIR, 'xgboost_ind.pkl')\n",
    "with open(classifier_path, 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f362b",
   "metadata": {},
   "source": [
    "### Step 5: Model Performance (Individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7ac03",
   "metadata": {},
   "source": [
    "Evaluate performance of classification model on labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1529802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read XGBoost model\n",
    "classifier_path = os.path.join(CHECKPOINT_DIR, 'xgboost_ind.pkl')\n",
    "with open(classifier_path, 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "# read normalization scaler\n",
    "scaler_path = os.path.join(CHECKPOINT_DIR, 'scaler_ind.pkl')\n",
    "with open(scaler_path, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# read labeled data\n",
    "df_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_feature_labeled.csv'))\n",
    "\n",
    "# normalize continuous features (z-scores)\n",
    "df_continuous = df_labeled[features_continuous].copy()\n",
    "X_test = scaler.transform(df_continuous)\n",
    "X_test = pd.DataFrame(X_test, columns=df_continuous.columns)\n",
    "# categorical features\n",
    "df_categorical = df_labeled[features_categorical].copy()\n",
    "X_test = pd.concat((X_test, df_categorical), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f1d4f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9857456140350878\n",
      "Precision (Fraud=1): 0.26666666666666666\n",
      "Recall (Fraud=1): 0.6666666666666666\n",
      "F1 (Fraud=1): 0.38095238095238093\n",
      "AUC: 0.8272626931567328\n",
      "\n",
      "Confusion matrix: [[TN, FP], [FN, TP]]\n",
      "[[895  11]\n",
      " [  2   4]]\n",
      "\n",
      "Full report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99       906\n",
      "         1.0       0.27      0.67      0.38         6\n",
      "\n",
      "    accuracy                           0.99       912\n",
      "   macro avg       0.63      0.83      0.69       912\n",
      "weighted avg       0.99      0.99      0.99       912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = classifier.predict(X_test)\n",
    "target = df_labeled['label']\n",
    "\n",
    "# accuracy\n",
    "a = accuracy_score(target, pred)\n",
    "print(\"Accuracy:\", a)\n",
    "\n",
    "# precision\n",
    "p = precision_score(target, pred)\n",
    "print(\"Precision (Fraud=1):\", p)\n",
    "\n",
    "# recall\n",
    "r = recall_score(target, pred)\n",
    "print(\"Recall (Fraud=1):\", r)\n",
    "\n",
    "# F1 score\n",
    "f1 = f1_score(target, pred)\n",
    "print(\"F1 (Fraud=1):\", f1)\n",
    "\n",
    "# AUC score\n",
    "auc = roc_auc_score(target, pred)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# confusion matrix\n",
    "m = confusion_matrix(target, pred)\n",
    "print(\"\\nConfusion matrix: [[TN, FP], [FN, TP]]\")\n",
    "print(m)\n",
    "\n",
    "cr = classification_report(target, pred)\n",
    "print(\"\\nFull report:\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be4d72",
   "metadata": {},
   "source": [
    "### Step 6: Add SHAP for Local Interpretability (Individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8e4ef",
   "metadata": {},
   "source": [
    "Generate explanation for each client with LFs and SHAPLEY\n",
    "- LFs that are satisfied\n",
    "- Top features based on SHAPLEY values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c29582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain classification model with SHAP\n",
    "explainer = shap.TreeExplainer(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef9d7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_lfs(customer_data):\n",
    "    \"\"\"Which rules fired?\"\"\"\n",
    "    fired_rules = []\n",
    "    for lf, lf_name in zip(lfs, lf_names):\n",
    "        if lf_name != \"\" and lf(customer_data) == POSITIVE:\n",
    "            fired_rules.append(lf_name)\n",
    "    return fired_rules\n",
    "\n",
    "def explain_features(transformed_data, threshold=0.5):\n",
    "    \"\"\"Top features based on SHAPLEY values\"\"\"\n",
    "    shap_values = explainer.shap_values(transformed_data.to_numpy().reshape(1, -1))\n",
    "    # crate tuples of features and scores\n",
    "    feature_impacts = [(f, float(s)) for f, s, in zip(features, shap_values[0])]\n",
    "    # keep features with absolute scores above 0.5\n",
    "    top_features = [(f, float(s)) for f, s in zip(features, shap_values[0]) if abs(s) > threshold]\n",
    "    return top_features\n",
    "\n",
    "def explain(customer_data, transformed_data, pred, threshold=0.5):\n",
    "    \"\"\"LFs that are satisfied anf top features based on SHAPLEY values\"\"\"\n",
    "    # lfs\n",
    "    lfs_fired = explain_lfs(customer_data)\n",
    "    # features with SHAP\n",
    "    features_important = explain_features(transformed_data, threshold)\n",
    "    features_description = []\n",
    "    # generate description for top features:\n",
    "    # e.g. high, low, satisfied, not satisfied\n",
    "    for f, v in features_important:\n",
    "        if v >= 0 and p == 1 or v < 0 and p == 0:\n",
    "            if v >= 0:\n",
    "                d = \"satisfied\" if f.startswith(('is_', 'has_')) else \"high\"\n",
    "            else:\n",
    "                d = \"not satisfied\" if f.startswith(('is_', 'has_')) else \"low\"\n",
    "            features_description.append((d, f, v))\n",
    "    return lfs_fired, features_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d10b400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted high-risk individuals\n",
      "----------\n",
      "Sample 31\n",
      "Fired LFs:\n",
      "- High transaction to income ratio\n",
      "Top features & SHAP values:\n",
      "- high income 5.34\n",
      "- high sum_amt_total 1.69\n",
      "- high channel_credit_volume 1.24\n",
      "----------\n",
      "Sample 42\n",
      "Fired LFs:\n",
      "- Short hold time\n",
      "Top features & SHAP values:\n",
      "- high channel_credit_volume 2.27\n",
      "- high median_hold_time_funds 2.27\n",
      "- high n_txn_total 2.34\n",
      "----------\n",
      "Sample 60\n",
      "Fired LFs:\n",
      "- Short hold time\n",
      "Top features & SHAP values:\n",
      "- high channel_credit_volume 2.41\n",
      "- high median_hold_time_funds 2.27\n",
      "- high n_txn_total 2.18\n",
      "----------\n",
      "Sample 108\n",
      "Fired LFs:\n",
      "Top features & SHAP values:\n",
      "- high income 5.02\n",
      "----------\n",
      "Sample 179\n",
      "Fired LFs:\n",
      "- Short hold time\n",
      "Top features & SHAP values:\n",
      "- high channel_credit_volume 2.49\n",
      "- high median_hold_time_funds 2.25\n",
      "- high n_txn_total 2.43\n",
      "----------\n",
      "Sample 246\n",
      "Fired LFs:\n",
      "- High transaction to income ratio\n",
      "Top features & SHAP values:\n",
      "- high income 3.7\n",
      "- high sum_amt_total 2.17\n",
      "- high channel_credit_volume 0.54\n",
      "----------\n",
      "Sample 389\n",
      "Fired LFs:\n",
      "- Short hold time\n",
      "Top features & SHAP values:\n",
      "- high channel_credit_volume 2.38\n",
      "- high median_hold_time_funds 2.24\n",
      "- high n_txn_total 2.42\n",
      "----------\n",
      "Sample 470\n",
      "Fired LFs:\n",
      "- Short hold time\n",
      "Top features & SHAP values:\n",
      "- high channel_credit_volume 2.46\n",
      "- high median_hold_time_funds 2.25\n",
      "- high n_txn_total 2.36\n",
      "----------\n",
      "Sample 482\n",
      "Fired LFs:\n",
      "- Short hold time\n",
      "Top features & SHAP values:\n",
      "- high channel_credit_volume 2.82\n",
      "- high median_hold_time_funds 2.81\n",
      "- high n_txn_total 1.61\n",
      "----------\n",
      "Sample 513\n",
      "Fired LFs:\n",
      "- High transaction to income ratio\n",
      "- High transaction to occupation median ratio\n",
      "Top features & SHAP values:\n",
      "- high income 4.41\n",
      "- high sum_amt_total 0.83\n",
      "- high channel_debit_volume 0.51\n",
      "- high txn_volume_vs_occupation_median 4.32\n",
      "----------\n",
      "Sample 615\n",
      "Fired LFs:\n",
      "- High transaction to income ratio\n",
      "Top features & SHAP values:\n",
      "- high income 4.8\n",
      "- high sum_amt_total 2.07\n",
      "- high channel_credit_volume 1.46\n",
      "----------\n",
      "Sample 623\n",
      "Fired LFs:\n",
      "- High wire transfers\n",
      "- Frequent wire transfers\n",
      "Top features & SHAP values:\n",
      "- high median_amt_wire 2.58\n",
      "- high wire_ratio 3.55\n",
      "- high age 2.46\n",
      "----------\n",
      "Sample 675\n",
      "Fired LFs:\n",
      "- High transaction to occupation median ratio\n",
      "Top features & SHAP values:\n",
      "- high txn_volume_vs_occupation_median 8.3\n",
      "- high age 1.97\n",
      "----------\n",
      "Sample 687\n",
      "Fired LFs:\n",
      "- High transaction to income ratio\n",
      "- High transaction to occupation median ratio\n",
      "- High wire transfers\n",
      "- Frequent wire transfers\n",
      "Top features & SHAP values:\n",
      "- high median_amt_wire 1.35\n",
      "- high wire_ratio 1.7\n",
      "- high income 2.25\n",
      "- high txn_volume_vs_occupation_median 4.01\n",
      "- high age 0.94\n",
      "----------\n",
      "Sample 741\n",
      "Fired LFs:\n",
      "- High transaction to income ratio\n",
      "Top features & SHAP values:\n",
      "- high income 5.84\n",
      "- high sum_amt_total 2.03\n",
      "- high channel_debit_volume 0.54\n",
      "- high channel_credit_volume 0.59\n"
     ]
    }
   ],
   "source": [
    "# read labeled data\n",
    "df_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_labeled.csv'))\n",
    "\n",
    "print(\"Predicted high-risk individuals\")\n",
    "for (i, customer_data), (i, transformed_data), p in zip(df_labeled.iterrows(), X_test.iterrows(), pred):\n",
    "    if p == 1:\n",
    "        print('-'*10)\n",
    "        print(f\"Sample {i}\")\n",
    "        lfs_fired, features_description = explain(customer_data, transformed_data, p)\n",
    "        print('Fired LFs:')\n",
    "        for l in lfs_fired:\n",
    "            print(\"-\", l)\n",
    "        print(\"Top features & SHAP values:\")\n",
    "        for d, f, v in features_description:\n",
    "            print(\"-\", d, f, round(v, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94cd2de",
   "metadata": {},
   "source": [
    "### Step 7: Get final output (Individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08e0e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master dataframes (individuals)\n",
    "df_ind_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_labeled.csv'))\n",
    "df_ind_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_labeled.csv'))\n",
    "# feature selected dataframes (individuals)\n",
    "df_ind_feat_labeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_feature_labeled.csv'))\n",
    "df_ind_feat_unlabeled = pd.read_csv(os.path.join(PROCESSED_DIR, 'individual_feature_labeled.csv'))\n",
    "# all data (individuals)\n",
    "df_ind = pd.concat((df_ind_labeled, df_ind_unlabeled))\n",
    "df_ind_feat = pd.concat((df_ind_feat_labeled, df_ind_feat_unlabeled), ignore_index=True)\n",
    "\n",
    "# normalize continuous features (z-scores)\n",
    "df_continuous = df_ind_feat[features_continuous].copy()\n",
    "df_ind_transformed = scaler.transform(df_continuous)\n",
    "df_ind_transformed = pd.DataFrame(df_ind_transformed, columns=df_continuous.columns)\n",
    "# categorical features\n",
    "df_categorical = df_ind_feat[features_categorical].copy()\n",
    "df_ind_transformed = pd.concat((df_ind_transformed, df_categorical), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7958aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted labels\n",
    "pred = classifier.predict(df_ind_transformed)\n",
    "\n",
    "# get explanation for each client\n",
    "explanation_list = []\n",
    "for (i, customer_data), (i, transformed_data), p in zip(df_ind.iterrows(), df_ind_transformed.iterrows(), pred):\n",
    "    lfs_fired, features_description = explain(customer_data, transformed_data, p)\n",
    "    explanation = \"\"\n",
    "    if len(lfs_fired) > 0:\n",
    "        explanation += ', '.join(lfs_fired) + '. '\n",
    "    if len(features_description) > 0:\n",
    "        explanation += \"Top features: \" + ', '.join([d+' '+f for d, f, _ in features_description])\n",
    "    explanation_list.append(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f097fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame({\n",
    "    'customer_id': df_ind['customer_id'],\n",
    "    'label': df_ind['label'],\n",
    "    'prediction': pred,\n",
    "    'explanation': explanation_list\n",
    "})\n",
    "df_final.to_csv(os.path.join(FINAL_DIR, 'individual_output.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
